{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import scipy\n",
    "from nltk.corpus import stopwords, words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from scipy import linalg\n",
    "from textblob import TextBlob\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total_actual_docs consists of cleaned and tagged data for english and spanish text. This is considered as input for the model.\n",
    "\n",
    "eg: but thou hast a few names in sardis that did not defile their garments and they shall walk with me in white for they are worthy __mas__ __tienes__ __unas__ __pocas__ __personas__ __en__ __sardis__ __que__ __no__ __han__ __ensuciado__ __sus__ __vestiduras__ __y__ __andar√°n__ __conmigo__ __en__ __vestiduras__ __blancas__ __porque__ __son__ __dignos__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"total_actual_docs.pkl\",'rb') as file1:\n",
    "    total_actual_docs = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have all the data, build LSI model to generate term matrix\n",
    "We can use either sklearn's CountVectorizer or TfidfVectorizer to calculate tfidf matrix. Here we will be using CountVectorizer and set min_df=10 because we want to ignore words that appear in less than 10 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word', encoding='utf-8', decode_error='ignore', min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "\n",
    "doc_vecs = count_vectorizer.fit_transform(total_actual_docs).transpose()\n",
    "doc_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate id2word using count_vec\n",
    "id2word = dict((v, k) for k, v in count_vec.vocabulary_.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a TFIDF transformer from our word counts (equivalent to \"fit\" in sklearn)\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a TFIDF vector for all documents from the original corpus (\"transform\" in sklearn)\n",
    "tfidf_corpus = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have tfidf and id2word matrices, we can build LSI model. Since my data is huge, I am going to use gensim distributed on a 32 core machine on AWS with a chunksize of 50000 to build LSI vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=100, chunksize= 50000, distributed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve vectors for the original tfidf corpus in the LSI space (\"transform\" in sklearn)\n",
    "lsi_corpus = lsi[tfidf_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperate English and Spanish terms from the original terms to get the term vectors and to calculate similarity matrix for each. id2word consists of all the terms in the original data with their corresponding indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_blobs = []\n",
    "for k,v in id2word.iteritems():\n",
    "    text_blobs.append(v)\n",
    "\n",
    "len(text_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sp_list = []\n",
    "en_list = []\n",
    "for word in text_blobs:\n",
    "    if word.startswith('__') or word.endswith('__'):\n",
    "        sp_list.append(word)\n",
    "    else:\n",
    "        en_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build similarity matrix for english and spanish terms\n",
    "# Get matrix of counts\n",
    "test_vecs_sp = count_vec.transform(sp_list).transpose()\n",
    "# Convert to gensim corpus\n",
    "test_corpus_sp = matutils.Sparse2Corpus(test_vecs_sp)\n",
    "# TFIDF transformation\n",
    "test_tfidf_sp = tfidf[test_corpus_sp]\n",
    "# LSI transformation\n",
    "test_lsi_sp = lsi[test_tfidf_sp]\n",
    "# Create an index transformer that calculates similarity based on our space\n",
    "test_index_sp = similarities.MatrixSimilarity(test_lsi_sp)\n",
    "test_index_sp.num_best = 5\n",
    "\n",
    "# Build LSI and matrix similarity for engish terms\n",
    "test_vecs_en = count_vec.transform(en_list).transpose()\n",
    "test_corpus_en = matutils.Sparse2Corpus(test_vecs_en)\n",
    "test_tfidf_en = tfidf[test_corpus_en]\n",
    "test_lsi_en = lsi[test_tfidf_en]\n",
    "test_index_en = similarities.MatrixSimilarity(test_lsi_en)\n",
    "test_index_en.num_best = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### term_folding_pipeline function is used to fold new terms into the existing LSI sapce to get the corresponding vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_folding_pipeline(term):\n",
    "    # Get matrix of counts\n",
    "    test_vecs1 = count_vec.transform(term).transpose()\n",
    "    # Convert to gensim corpus\n",
    "    test_corpus1 = matutils.Sparse2Corpus(test_vecs1)\n",
    "    # TFIDF transformation\n",
    "    test_tfidf1 = tfidf[test_corpus1]\n",
    "    # LSI transformation\n",
    "    test_lsi1 = lsi[test_tfidf1]\n",
    "    return test_lsi1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create a english to spanish dictionary by getting terms with highest similarity scores. Similarly we can compute spanish to english dictionary too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translation_new_en = defaultdict(list)\n",
    "for word in en_list:\n",
    "    if len(word) < 1:\n",
    "        continue\n",
    "    en_term_lsi = term_folding_pipeline([word])\n",
    "    translation_list = []\n",
    "    score = test_index_sp[en_term_lsi]\n",
    "    for i in range(len(score[0])):\n",
    "        translation_list.append(sp_list[score[0][i][0]].strip('__'))\n",
    "    translation_new_en[word] = translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
